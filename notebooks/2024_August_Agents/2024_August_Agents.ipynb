{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqhMgAGwmLXc"
      },
      "source": [
        "# Deeper Contexts: LangGraph + Content-Centric Knowledge Graphs\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Knowledge graphs are a popular for use in RAG (Retrieval Augmented Generation). These techniques often involve extracting the structured graph -- entities and relationships -- using an LLM, and then retrieving information from the knowledge graph and feeding it to the LLM as context for answering questions.\n",
        "\n",
        "We've introduced [content-centric knowledge graphs](...) as an improvement to knowledge graphs focused on RAG. Rather than extracting detailed structure, a content-centric knowledge graph adds links between vector chunks, providing the ability to navigate links for deeper information without incurring an expensive and hands-on indexing process. We implemented this concept in LangChain as [GraphVectorStore](...).\n",
        "\n",
        "One huge benefit of the knowledge graph is allowing the LLM to explore information in much the same way we would. While traversals allow exploring part of the graph to answer a specific question, another important capability is the ability to ask follow-up questions. [Knowledge Graph Prompting for Multi-Document Question Answering](https://arxiv.org/abs/2308.11730) demonstrated the benefits of being able to limit follow-up questions using the graph structure.\n",
        "\n",
        "As an example -- if you read a section of a book and don't understand some of the terms, is it better to consult a dictionary or that books glossary? If you go to the dictionary, you *may* find the answer and be able to relate it to what you read... but if you look in the glossary, you'll get the definition that is used within the book, making it much easier to understand the context of what you have already read.\n",
        "\n",
        "In this post we'll use LangGraph to build an agent and provide it with tools to perform an initial retrieval (using [MMR graph traversal](...)) as well as a tool for asking a follow-up question \"in the neighborhood\" of already retrieved documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6dFtwI_xmFW"
      },
      "source": [
        "## Colab Environment Setup\n",
        "\n",
        "The following block will configure the environment from the Colab Secrets.\n",
        "To run it, you should have the following Colab Secrets defined and accessible to this notebook:\n",
        "\n",
        "- `OPENAI_API_KEY`: The OpenAI key.\n",
        "- `ASTRA_DB_DATABASE_ID`: The Astra DB database ID.\n",
        "- `ASTRA_DB_APPLICATION_TOKEN`: The Astra DB Application token.\n",
        "- `LANGCHAIN_API_KEY`: Optional. If defined, will enable LangSmith tracing.\n",
        "- `ASTRA_DB_KEYSPACE`: Optional. If defined, will specif the Astra DB keyspace. If not defined, will use thee default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0-5VJWGsBM3"
      },
      "outputs": [],
      "source": [
        "# Install modules.\n",
        "%pip install -U -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "2o4XjqFHyFE_",
        "outputId": "ba45c715-2a0a-411e-c812-2e790d056e91"
      },
      "outputs": [],
      "source": [
        "# Override knowledge store from my fork.\n",
        "# Won't be needed once this is available.\n",
        "%pip install --force-reinstall git+https://github.com/bjchambers/langchain.git@cassandra-graph-vectorstore-kwargs#subdirectory=libs/community\n",
        "%pip install --force-reinstall git+https://github.com/datastax/ragstack-ai.git@timeout#subdirectory=libs/knowledge-store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure import paths.\n",
        "import sys\n",
        "sys.path.append(\"../../\")\n",
        "\n",
        "# Initialize environment variables.\n",
        "from utils import initialize_environment\n",
        "initialize_environment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAQb3xW_zjC0",
        "outputId": "190a1f0e-cf9e-4ff4-b45c-6b9dad58d1c7"
      },
      "outputs": [],
      "source": [
        "#@ GraphVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.graph_vectorstores.cassandra import CassandraGraphVectorStore\n",
        "import cassio\n",
        "\n",
        "cassio.init(auto=True)\n",
        "store = CassandraGraphVectorStore(\n",
        "    embedding = OpenAIEmbeddings(),\n",
        "    node_table=\"neighborhood_nodes\",\n",
        "    insert_timeout = 1000.0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DCj52pdDPjp"
      },
      "source": [
        "## Load Initial Data\n",
        "\n",
        "This only needs to be done once to populate the datastore. Since the focus of this notebook is on using the knowledge graph within an agent, I'll just do all of the loading in a single code block, below. Refer to the comments if interseted, and/or the documentation for populating a `GraphVectorStore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u4lD-AqDMMs"
      },
      "outputs": [],
      "source": [
        "#@ Load Data Into the Graph VectorStore\n",
        "from datasets.wikimultihop.load import load_2wikimultihop\n",
        "load_2wikimultihop(store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vk3AglwohCl"
      },
      "source": [
        "## The Tools\n",
        "\n",
        "Each of the tools will be set up to return a Pydantic model representing the retrieved document chunks. Each chunk will include the `chunk_id`, the `document_id` (the document the chunk is from), and the `page_content`.\n",
        "\n",
        "The initial retrieval tool will take the question and return the representative chunks. The neighborhood retrieval tool will take a question and a list of `document_ids`. It will retrieve nodes matching the question from the graph starting with those adjacent to `document_ids`.\n",
        "\n",
        "This combination allows the agent to formulate one or more initial questions based on the user's request, and then ask follow-up questions as needed to better understand the information retrieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiujcZ68pUjH"
      },
      "outputs": [],
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.tools import StructuredTool\n",
        "from typing import List, Sequence, Any\n",
        "\n",
        "class InitialRetrieveInput(BaseModel):\n",
        "    question: str = Field(description=\"Question to retrieve content for. Should be a simple question describing the starting point for retrieval likely to have content.\")\n",
        "\n",
        "class FollowupRetrieveInput(BaseModel):\n",
        "    question: str = Field(descri    ption=\"Question to retrieve follow-up content for.\")\n",
        "    neighborhood: List[str] = Field(default = None, description=\"Content IDs that should be used to limit the follow-up retrieval. Only information linked from the neighborhood will be retrieved.\")\n",
        "\n",
        "# TODO: We could also have a follow-up that just limits to the *source* IDs.\n",
        "# TODO: We could combine these two tools with an `Optional[List[str]]` neighborhood.\n",
        "\n",
        "class RetrievedContent(BaseModel):\n",
        "    content_id: str = Field(description=\"ID of this chunk of content\")\n",
        "    content: str = Field(description = \"Content of this chunk\")\n",
        "\n",
        "class RetrievedContext(BaseModel):\n",
        "    question: str = Field(description = \"question this context was answering\")\n",
        "    contents: List[RetrievedContent] = Field(description=\"retrieved content\")\n",
        "\n",
        "def _retrieve(question: str,\n",
        "              *,\n",
        "              neighborhood: Sequence[str] = (),\n",
        "              **kwargs: Any) -> RetrievedContext:\n",
        "    documents = store.search(\n",
        "        query = question,\n",
        "        search_type = \"mmr_traversal\",\n",
        "        initial_roots = neighborhood,\n",
        "        fetch_k = 0 if neighborhood else 100,\n",
        "        **kwargs,\n",
        "    )\n",
        "    return RetrievedContext(\n",
        "        question = question,\n",
        "        contents = [\n",
        "            RetrievedContent(content_id=doc.id, content=doc.page_content)\n",
        "            for doc in documents\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def initial_retrieve(question: str) -> RetrievedContext:\n",
        "    return _retrieve(question, depth=0)\n",
        "\n",
        "def followup_retrieve(question: str, neighborhood: List[str]) -> RetrievedContext:\n",
        "    return _retrieve(question, neighborhood=neighborhood)\n",
        "\n",
        "initial_retrieve_tool = StructuredTool.from_function(\n",
        "    func=initial_retrieve,\n",
        "    name=\"InitialRetrieve\",\n",
        "    description=\"Retrieve context answering a specific question. Use when there isn't a known neighborhood to search in.\",\n",
        "    args_schema=InitialRetrieveInput,\n",
        "    return_direct=False,\n",
        "    # coroutine= ... <- you can specify an async method if desired as well\n",
        ")\n",
        "\n",
        "followup_retrieve_tool = StructuredTool.from_function(\n",
        "    func=followup_retrieve,\n",
        "    name=\"FollowupRetrieve\",\n",
        "    description=\"Retrieve context answering a follow-up question using information near existing content. Only use when there are likely links from existing content to the desired follow-up content.\",\n",
        "    args_schema=FollowupRetrieveInput,\n",
        "    return_direct=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhF7H50bofBz"
      },
      "source": [
        "## The Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v673EXdIr7pz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "tools = [\n",
        "    initial_retrieve_tool,\n",
        "    followup_retrieve_tool,\n",
        "]\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "prompt.messages\n",
        "\n",
        "# Create the agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "\n",
        "# Create the agent executor (the actual runnable)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example 1: No follow-up questions needed\n",
        "We use a question from the 2wikimultihop test set -- specifically \"Where was the place of death of the directory of the film Ladies Courageous?\".\n",
        "This normally requires multiple hops -- first ask who directed the film \"Ladies Courageous\", then ask where John Rawlins died."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ29VFHH_jEX",
        "outputId": "fb78ac7a-35fe-4292-f86b-23e9aca7cfcc"
      },
      "outputs": [],
      "source": [
        "store.store._session.default_timeout = 60.0\n",
        "agent_executor.invoke({\"input\": \"Where was the place of death of the director of the film Ladies Courageous?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the output, we see that the agent retrieved results for an initial question -- \"Who was the director of the film Ladies Courageous\".\n",
        "The results included the Wikipedia page about Ladies Courageous, but also included the page about John Rawlins.\n",
        "This is because it was linked to by the first page, and the MMR graph retrieval determined the results were relevant (John Rawlins was the directory).\n",
        "\n",
        "This is interesting, because it didn't require a second hop at all, thanks to the graph edges!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Multiple Hops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_executor.invoke({\"input\": \"Which film has the director who died earlier, The Boy Turns Man or A Strange Adventure?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v11fND0Qolif"
      },
      "source": [
        "## Conclusion\n",
        "To be written."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
