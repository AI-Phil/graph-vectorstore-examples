{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install modules\n",
    "%pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure import paths.\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Initialize environment variables.\n",
    "from utils import initialize_environment\n",
    "initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ GraphVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.graph_vectorstores.cassandra import CassandraGraphVectorStore\n",
    "import cassio\n",
    "\n",
    "cassio.init(auto=True)\n",
    "store = CassandraGraphVectorStore(\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    node_table=\"neighborhood_nodes\",\n",
    "    insert_timeout = 1000.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Load Data Into the Graph VectorStore\n",
    "if input(\"load data (y/N): \").lower() == \"y\":\n",
    "    print(\"Loading data...\")\n",
    "    from datasets.wikimultihop.load import load_2wikimultihop\n",
    "    load_2wikimultihop(store)\n",
    "else:\n",
    "    print(\"Skipped loading data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.graph_vectorstores.links import get_links\n",
    "from typing import Iterable\n",
    "\n",
    "def communities(documents: Iterable[Document]) -> Iterable[Iterable[Document]]:\n",
    "    \"\"\"Group documents by community inferred from the links.\"\"\"\n",
    "    import networkx as nx\n",
    "    from networkx.algorithms.community.centrality import girvan_newman\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # First pass -- map from tag to noed IDs with that incoming.\n",
    "    documents_by_id = {}\n",
    "    documents_by_incoming = {}\n",
    "    for document in documents:\n",
    "        # Add the node to the graph\n",
    "        graph.add_node(document.id)\n",
    "        documents_by_id[document.id] = document\n",
    "\n",
    "        # Record the incoming edges.\n",
    "        for link in get_links(document):\n",
    "            if link.direction == \"in\" or link.direction == \"bidir\":\n",
    "                documents_by_incoming.setdefault((link.kind, link.tag), set()).add(document.id)\n",
    "\n",
    "    # Second pass -- add edges for each outgoing edge.\n",
    "    for document in documents:\n",
    "        for link in get_links(document):\n",
    "            if link.direction == \"out\" or link.direction == \"bidir\":\n",
    "                for target in documents_by_incoming.get((link.kind, link.tag), set()):\n",
    "                    graph.add_edge(document.id, target)\n",
    "\n",
    "    # Find communities and output documents grouped by community.\n",
    "    communities = girvan_newman(graph)\n",
    "    return [[documents_by_id[id] for id in community] for community in communities]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "1. Fetch many chunks, group by community, map summarization on communities, reduce to a single summary.\n",
    "   (See https://python.langchain.com/v0.2/docs/tutorials/summarization/#orchestration-via-langgraph)\n",
    "2. Demonstrate on a dateset / write text"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
