{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Knowledge Graphs at Production Scale\n",
    "Using Knowledge Graphs to improve the results of Retrieval-Augmented Generation (RAG) applications is widely discussed. Most examples demonstrate how to build a knowledge graph using a relatively small number of documents. This may be because the typical approach – extracting fine-grained, entity-centric information just doesn’t scale. Running each document through a model to extract the entities (nodes) and relationships (edges) takes too long (and costs too much) to run on large datasets.\n",
    "\n",
    "We’ve talked about the idea of content-centric knowledge graphs – a vector-store allowing links between chunks – as an easier to use and more efficient approach. In this post we put that to the test. We load a subset of the wikipedia articles from the [2wikimultihop](https://github.com/Alab-NII/2wikimultihop) dataset using both techniques and discuss what this means for loading the entire dataset. We demonstrate the results of some questions over the loaded data. We’ll also load the entire dataset – nearly 6 million documents – into a content-centric [GraphVectorStore](https://www.datastax.com/blog/now-in-langchain-graph-vector-store-add-structured-data-to-rag-apps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: langchain-core==0.2.27 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 2)) (0.2.27)\n",
      "Requirement already satisfied: langchain-community==0.2.11 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 3)) (0.2.11)\n",
      "Requirement already satisfied: langchain-openai==0.1.20 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 4)) (0.1.20)\n",
      "Requirement already satisfied: langchainhub==0.1.21 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 5)) (0.1.21)\n",
      "Requirement already satisfied: langsmith==0.1.99 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 6)) (0.1.99)\n",
      "Requirement already satisfied: ragstack-ai-knowledge-store<0.3,>=0.2.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: simsimd<6.0.0,>=5.0.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 8)) (5.4.3)\n",
      "Requirement already satisfied: tqdm<4.67,>=4.66.4 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 9)) (4.66.5)\n",
      "Requirement already satisfied: networkx in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 10)) (3.3)\n",
      "Requirement already satisfied: langchain_experimental in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.0.64)\n",
      "Collecting langchain_experimental (from -r requirements.txt (line 3))\n",
      "  Using cached langchain_experimental-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting neo4j (from -r requirements.txt (line 4))\n",
      "  Downloading neo4j-5.25.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (3.10.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.12 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (0.2.12)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (1.40.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchainhub==0.1.21->-r ../../common_requirements.txt (line 5)) (2.32.0.20240712)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langsmith==0.1.99->-r ../../common_requirements.txt (line 6)) (3.10.7)\n",
      "Requirement already satisfied: cassio<0.2.0,>=0.1.7 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (0.1.8)\n",
      "INFO: pip is looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_experimental (from -r requirements.txt (line 3))\n",
      "  Using cached langchain_experimental-0.3.1.post1-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Using cached langchain_experimental-0.3.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Using cached langchain_experimental-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Using cached langchain_experimental-0.0.65-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pytz (from neo4j->-r requirements.txt (line 4))\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: cassandra-driver<4.0.0,>=3.28.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from cassio<0.2.0,>=0.1.7->ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (3.29.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.12->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2024.7.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (2024.7.24)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.7->ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (0.2.1.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.7->ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (8.1.7)\n",
      "Requirement already satisfied: six in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.7->ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (1.16.0)\n",
      "Downloading neo4j-5.25.0-py3-none-any.whl (296 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Installing collected packages: pytz, neo4j\n",
      "Successfully installed neo4j-5.25.0 pytz-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#@ Install modules\n",
    "%pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Configure import paths.\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Initialize environment variables.\n",
    "from utils import initialize_environment\n",
    "initialize_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to Load\n",
    "For this notebook, we'll work on loading the first 100 articles from Wikipedia. We use Wikipedia data from the [2wikimultihop](https://github.com/Alab-NII/2wikimultihop) dataset. To execute the rest of the notebook, you will need to download [para_with_hyperlink.zip](https://www.dropbox.com/s/wlhw26kik59wbh8/para_with_hyperlink.zip) to the `wikimultihop` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from datasets.wikimultihop.load import wikipedia_lines\n",
    "\n",
    "NUM_LINES_TO_LOAD = 100\n",
    "lines_to_load = list(islice(wikipedia_lines(), NUM_LINES_TO_LOAD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Centric: LLMGraphTrasnformer\n",
    "\n",
    "Loading documents into an entity-centric graph store like Neo4j was done using LangChain’s `LLMGraphTransformer`. The code is based on LangChain's [\"How to construct knowledge graphs\"](https://python.langchain.com/docs/how_to/graph_constructing/#llm-graph-transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (but NOT written) 100 in 405.93s\n",
      "OpenAI stats: prompt tokens 116180, completion tokens 27081, total cost 0.9871149999999999\n"
     ]
    }
   ],
   "source": [
    "#@ Extract GraphDocuments\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "from time import perf_counter\n",
    "start = perf_counter()\n",
    "\n",
    "documents_to_load = [Document(page_content=line) for line in lines_to_load]\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    graph_documents = llm_transformer.convert_to_graph_documents(documents_to_load)\n",
    "    end = perf_counter()\n",
    "\n",
    "    print(f\"Loaded (but NOT written) {NUM_LINES_TO_LOAD} in {end - start:0.2f}s\")\n",
    "    print(f\"OpenAI stats: prompt tokens {cb.prompt_tokens}, completion tokens {cb.completion_tokens}, total cost {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written in 2.39s\n"
     ]
    }
   ],
   "source": [
    "#@ Write GraphDocuments to Neo4j\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "from time import perf_counter\n",
    "start = perf_counter()\n",
    "\n",
    "entity_centric_store = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=\"update-boxer-percent-slang-salad-9579\")\n",
    "entity_centric_store.add_graph_documents(graph_documents)\n",
    "\n",
    "end = perf_counter()\n",
    "print(f\"Written in {end - start:0.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Centric: GraphVectorStore\n",
    "Loading the data into `GraphVectorStore` is roughly the same as loading it into a vector store. The only addition is that we compute metadata indicating how the pages link to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Configure Tables\n",
    "import cassio\n",
    "cassio.init(auto=True)\n",
    "TABLE_NAME = \"wiki_load\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#@ Empty the table (optional)\n",
    "if input(\"clear data(y/N): \").lower() == \"y\":\n",
    "    print(\"Clearing data...\")\n",
    "    from cassio.config import check_resolve_session, check_resolve_keyspace\n",
    "    session = check_resolve_session()\n",
    "    keyspace = check_resolve_keyspace()\n",
    "\n",
    "    session.execute(f\"TRUNCATE TABLE {keyspace}.{TABLE_NAME};\")\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Skipped clearing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Create GraphVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.graph_vectorstores.cassandra import CassandraGraphVectorStore\n",
    "\n",
    "content_centric_store = CassandraGraphVectorStore(\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    node_table=TABLE_NAME,\n",
    "    insert_timeout = 1000.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Add links to documents\n",
    "import json\n",
    "from langchain_core.graph_vectorstores.links import METADATA_LINKS_KEY, Link\n",
    "\n",
    "def parse_document(line: str) -> Document:\n",
    "    para = json.loads(line)\n",
    "\n",
    "    id = para[\"id\"]\n",
    "    links = {\n",
    "        Link.outgoing(kind=\"href\", tag=id)\n",
    "        for m in para[\"mentions\"]\n",
    "        if m[\"ref_ids\"] is not None\n",
    "        for id in m[\"ref_ids\"]\n",
    "    }\n",
    "    links.add(Link.incoming(kind=\"href\", tag=id))\n",
    "    return Document(\n",
    "        id = id,\n",
    "        page_content = \" \".join(para[\"sentences\"]),\n",
    "        metadata = {\n",
    "            \"content_id\": para[\"id\"],\n",
    "            METADATA_LINKS_KEY: list(links)\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading entity-centric data...\n",
      "Loaded (and written) in 1.43s\n"
     ]
    }
   ],
   "source": [
    "#@ Load Data Into GraphVectorStore\n",
    "print(\"Loading entity-centric data...\")\n",
    "from time import perf_counter\n",
    "\n",
    "start = perf_counter()\n",
    "kg_documents = [parse_document(line) for line in lines_to_load]\n",
    "content_centric_store.add_documents(kg_documents)\n",
    "end = perf_counter()\n",
    "print(f\"Loaded (and written) {NUM_LINES_TO_LOAD} in {end - start:0.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I run this, it takes about 1.43s to load these 100 documents. Under the hood, this extracts links from the wikipedia page. I have previously run all 5,989,847 documents from the dump through this process using async, and it took about 2.5 hours total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Benchmarks\n",
    "Running at 100 rows, the entity-centric approach using gpt-4o took 405.93s to extract the `GraphDocumuent`s and 10.99s to write them to Neo4j, while the content-centric approach takes 1.43s. The content-centric approach would be projected to take about 24 hours to load the entire 5,989,847 documents, but thanks to parallelism when I run the whole procedure it takes only 2.5 hours. Loading all 5,989,847 pages with the entity-centric approach would be projected to take 41 weeks (`(405.93 * 10.99) / 100 * 5989847 / 60 / 60 / 24 / 7`). Even with similar parallelism benefits, that would take over 4 weeks.\n",
    "\n",
    "I didn't actually construct the entire knowledge graph, because the costs to load 100 documents using gpt-4o was $0.9871149999999999. Loading the nearly 6 million documents would have cost an estimated $58,700 assuming everything worked the first time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Answers\n",
    "TODO: Demonstrate questions over the graph store. Show two specific questions and the corresponding answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages/langchain/hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "#@ VectorGraphStore RAG chain\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "content_centric_retriever = content_centric_store.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "content_centric_chain = (\n",
    "    {\"context\": content_centric_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "entity_centric_chain = GraphCypherQAChain.from_llm(graph=entity_centric_store, llm=llm, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Centric\n",
      "--------------\n",
      "Question 1: When was 'The Circle' released?\n",
      "{'query': \"When was 'The Circle' released?\", 'result': \"I don't know the answer.\"}\n",
      "\n",
      "Question 2: Where is Urup located?\n",
      "{'query': 'Where is Urup located?', 'result': \"I don't know the answer.\"}\n",
      "Entity Centric Time in 2.02s\n",
      "OpenAI stats: prompt tokens 7324, completion tokens 80, total cost 0.03782\n",
      "\n",
      "Content Centric\n",
      "---------------\n",
      "Question 1: When was 'The Circle' released?\n",
      "'The Circle' was released in 1988.\n",
      "\n",
      "Question 2: Where is Urup located?\n",
      "Urup is located in Badakhshan Province in north-eastern Afghanistan.\n",
      "Content Centric Time in 2.39s\n",
      "OpenAI stats: prompt tokens 450, completion tokens 26, total cost 0.00264\n"
     ]
    }
   ],
   "source": [
    "QUESTION1 = \"When was 'The Circle' released?\"\n",
    "QUESTION2 = \"Where is Urup located?\"\n",
    "\n",
    "print(\"Entity Centric\\n--------------\")\n",
    "start = perf_counter()\n",
    "with get_openai_callback() as cb:\n",
    "    print(f\"Question 1: {QUESTION1}\")\n",
    "    print(entity_centric_chain.invoke(QUESTION1))\n",
    "    print(f\"\\nQuestion 2: {QUESTION2}\")\n",
    "    print(entity_centric_chain.invoke(QUESTION2))\n",
    "\n",
    "    end = perf_counter()\n",
    "    print(f\"Entity Centric Time in {end - start:0.2f}s\")\n",
    "    print(f\"OpenAI stats: prompt tokens {cb.prompt_tokens}, completion tokens {cb.completion_tokens}, total cost {cb.total_cost}\")\n",
    "\n",
    "print(\"\\nContent Centric\\n---------------\")\n",
    "start = perf_counter()\n",
    "with get_openai_callback() as cb:\n",
    "    print(f\"Question 1: {QUESTION1}\")\n",
    "    print(content_centric_chain.invoke(QUESTION1))\n",
    "    print(f\"\\nQuestion 2: {QUESTION2}\")\n",
    "    print(content_centric_chain.invoke(QUESTION2))\n",
    "\n",
    "    end = perf_counter()\n",
    "    print(f\"Content Centric Time in {end - start:0.2f}s\")\n",
    "    print(f\"OpenAI stats: prompt tokens {cb.prompt_tokens}, completion tokens {cb.completion_tokens}, total cost {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Extracting fine-grained, entity-specific knowledge graphs is time and cost prohibitive at scale. When asked questions over the subset of data that was loaded, the additional granularity (and extra cost loading the fine-grained graph) returned more tokens to include the prompt, but generated useless answers!\n",
    "\n",
    "`GraphVectorStore` takes a coarse-grained, content-centric approach that makes it fast and easy to build a knowledge graph. You can start with your existing code for populating a `VectorStore` using LangChain and add links (edges) between chunks to improve the retrieval process.\n",
    "\n",
    "Graph RAG is a useful tool for enabling GenAI RAG applications to retrieve more deeply relevant context. But using a fine-grained, entity-centric approach does not scale to production needs. If you're looking to add knowledge graph capabilities to your RAG application, try `GraphVectorStore`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
