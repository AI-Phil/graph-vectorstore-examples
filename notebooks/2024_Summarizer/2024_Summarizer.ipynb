{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key idea in Microsoft’s [Graph RAG for Summarization](https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/) is to group content by community prior to summarizing. This technique may be applied without storing everything as a knowledge graph. In this post we’ll demonstrate a generic LangChain summarizer which returns community summaries based on extracted links between documents. We'll apply to this retrieval results both with and without persisted links. The same techniques could be applied to summarize communities in large documents during indexing without building an explicit graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background: From Local to Global\n",
    "\n",
    "In [“From Local to Global: A Graph RAG Approach to Query-Focused Summarization”](https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/), Microsoft presents a technique for answering queries over an entire dataset. The idea boils down to extracting an entity knowledge graph from the documents and pre-generating community summaries for groups of closely related entities. The community summaries are then used for generating initial per-community responses to a question which are then combined (hierarchically) to generate the final answer.\n",
    "\n",
    "The use of communities for summarization has wider applicability than just a starting point for global summarization. Anytime the context exceeds the maximum length supported by the LLM, summarization may be necessary. Similarly, anytime the content to be summarized exceeds the content length it will need to be done on some subgroups. In both of these cases, grouping the content into communities first is likely to improve the results, since each of the initial calls receives information related to a specific topic (based on the community).\n",
    "\n",
    "Consider a case where you have 10 documents about topic A and 10 documents about topic B. If we divide this into groups of 10 documents, we could evenly divide these – including 5 documents from topic A and 5 documents from topic B in each group. In this case, there is a risk the generated summary only addresses one of the topics. Even if it doesn’t, we’re likely generating output pertaining to each topic multiple times. If we instead group the documents by topic, we generate a summary of everything related to A and a summary of everything related to B (which should have minimal overlap) and then a combined summary. This is the benefit of community summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: langchain-core==0.2.27 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 2)) (0.2.27)\n",
      "Requirement already satisfied: langchain-community==0.2.11 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 3)) (0.2.11)\n",
      "Requirement already satisfied: langchain-openai==0.1.20 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 4)) (0.1.20)\n",
      "Requirement already satisfied: langchainhub==0.1.21 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 5)) (0.1.21)\n",
      "Requirement already satisfied: langsmith==0.1.99 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 6)) (0.1.99)\n",
      "Requirement already satisfied: ragstack-ai-knowledge-store<0.3,>=0.2.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: simsimd<6.0.0,>=5.0.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 8)) (5.3.0)\n",
      "Requirement already satisfied: tqdm<4.67,>=4.66.4 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r ../../common_requirements.txt (line 9)) (4.66.5)\n",
      "Requirement already satisfied: langgraph in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.2.16)\n",
      "Collecting langgraph (from -r requirements.txt (line 3))\n",
      "  Downloading langgraph-0.2.22-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: networkx in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (3.10.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.12 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (0.2.12)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (1.40.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchainhub==0.1.21->-r ../../common_requirements.txt (line 5)) (2.32.0.20240712)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langsmith==0.1.99->-r ../../common_requirements.txt (line 6)) (3.10.7)\n",
      "Requirement already satisfied: cassio<0.2.0,>=0.1.7 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (0.1.8)\n",
      "INFO: pip is looking at multiple versions of langgraph to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langgraph-0.2.21-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langgraph-0.2.20-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langgraph-0.2.19-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langgraph-0.2.18-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached langgraph-0.2.17-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langgraph->-r requirements.txt (line 3)) (1.0.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: cassandra-driver<4.0.0,>=3.28.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from cassio<0.2.0,>=0.1.7->ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (3.29.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.12->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core==0.2.27->-r ../../common_requirements.txt (line 2)) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (2024.7.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (2024.7.24)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.7->ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (0.2.1.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai==0.1.20->-r ../../common_requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.11->-r ../../common_requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.7->ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (8.1.7)\n",
      "Requirement already satisfied: six in /Users/benjamin.chambers/code/graph-vectorstore-examples/.venv/lib/python3.11/site-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.7->ragstack-ai-knowledge-store<0.3,>=0.2.1->-r ../../common_requirements.txt (line 7)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#@ Install modules\n",
    "%pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Configure import paths.\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Initialize environment variables.\n",
    "from utils import initialize_environment\n",
    "initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Create GraphVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.graph_vectorstores.cassandra import CassandraGraphVectorStore\n",
    "import cassio\n",
    "\n",
    "cassio.init(auto=True)\n",
    "store = CassandraGraphVectorStore(\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    node_table=\"summarize\",\n",
    "    insert_timeout = 1000.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassio.config import check_resolve_session, check_resolve_keyspace\n",
    "session = check_resolve_session()\n",
    "keyspace = check_resolve_keyspace()\n",
    "\n",
    "\n",
    "session.execute(f\"TRUNCATE TABLE {keyspace}.summarize;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The following loads Wikipedia data from the [2wikimultihop](https://github.com/Alab-NII/2wikimultihop) dataset. To execute it, you will need to download [para_with_hyperlink.zip](https://www.dropbox.com/s/wlhw26kik59wbh8/para_with_hyperlink.zip) to the `wikimultihop` directory. The code is setup to automatically resume from where it previously made in case you encounter a transient failure and need to restart. On my machine it takes about 2.5 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Resuming loading with 4972 completed, 1018 remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1018/1018 [20:20<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "#@ Load Data Into the Graph VectorStore\n",
    "\n",
    "if input(\"load data (y/N): \").lower() == \"y\":\n",
    "    print(\"Loading data...\")\n",
    "    from datasets.wikimultihop.load import load_2wikimultihop\n",
    "    load_2wikimultihop(store)\n",
    "else:\n",
    "    print(\"Skipped loading data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Community Summarization, In Memory\n",
    "\n",
    "Detecting communities can be done easily using a library such as `networkx`. In the following code, LangChain `Documents` are turned into nodes, with edges based on their `link` properties. This allows the documents to be grouped by community for accurate summarization.\n",
    "\n",
    "The following also includes a `CommunitySummarizer` which can be used to automatically extract links from documents (based on keywords, named entities, links, etc.) as part of building the graph. This allows using the community summarization even on chunks that don't already contain links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.graph_vectorstores.links import get_links, copy_with_links\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_community.graph_vectorstores.extractors.link_extractor import (\n",
    "    LinkExtractor,\n",
    ")\n",
    "from typing import Any, Dict, Iterable, List, Set, Tuple\n",
    "import networkx as nx\n",
    "\n",
    "def _best_communities(graph: nx.DiGraph) -> Tuple[Set[str]] | None:\n",
    "    \"\"\"Compute the best communities.\n",
    "\n",
    "    Iteratively applies Girvan-Newman algorithm as long as the modularity improves.\n",
    "\n",
    "    Returns:\n",
    "        The communities from the last iteration of the Girvan-Newman algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Also continue running until the size of communities is below\n",
    "    # a specified threshold?\n",
    "\n",
    "    best_modularity = float(\"-inf\")\n",
    "    best_communities = None\n",
    "    for new_communities in nx.algorithms.community.girvan_newman(graph):\n",
    "        new_modularity = nx.algorithms.community.modularity(graph, new_communities)\n",
    "        if new_modularity > best_modularity:\n",
    "            best_modularity = new_modularity\n",
    "            best_communities = new_communities\n",
    "        else:\n",
    "            break\n",
    "    return best_communities\n",
    "\n",
    "def group_by_community(documents: Iterable[Document]) -> List[List[Document]]:\n",
    "    \"\"\"Group documents by community inferred from the links.\"\"\"\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # First pass -- map from tag to noed IDs with that incoming.\n",
    "    documents_by_id = {}\n",
    "    documents_by_incoming = {}\n",
    "    for document in documents:\n",
    "        # Add the node to the graph\n",
    "        graph.add_node(document.id)\n",
    "        documents_by_id[document.id] = document\n",
    "\n",
    "        # Record the incoming edges.\n",
    "        for link in get_links(document):\n",
    "            if link.direction == \"in\" or link.direction == \"bidir\":\n",
    "                documents_by_incoming.setdefault((link.kind, link.tag), set()).add(document.id)\n",
    "\n",
    "\n",
    "    # Second pass -- add edges for each outgoing edge.\n",
    "    for document in documents_by_id.values():\n",
    "        for link in get_links(document):\n",
    "            if link.direction == \"out\" or link.direction == \"bidir\":\n",
    "                for target in documents_by_incoming.get((link.kind, link.tag), set()):\n",
    "                    graph.add_edge(document.id, target)\n",
    "\n",
    "    # Find communities and output documents grouped by community.\n",
    "    # The algorithm returns an iterator over iterations.\n",
    "    # Iterate until the modularity no longer increases.\n",
    "    return [\n",
    "        [documents_by_id[id] for id in community]\n",
    "        for community in _best_communities(graph)\n",
    "    ]\n",
    "\n",
    "class CommunitySummarizer:\n",
    "  def __init__(self,\n",
    "               summarize: Runnable[Dict[str, Any], Any],\n",
    "               *,\n",
    "               link_extractors: Tuple[LinkExtractor[Document]] = ()) -> None:\n",
    "    \"\"\"Create a community summarizer.\n",
    "\n",
    "    Parameters:\n",
    "      summarize: Chain to use for summarization. Must accept documents to\n",
    "          be summarized in the `\"context\"` key. Other arguments may be passed\n",
    "          to the summarizaiton chain when `summarize` is invoked.\n",
    "      link_extractors: If specified, links will be added to input documents\n",
    "          using the link extractors. If empty, only the links already present\n",
    "          in the documents will be used.\n",
    "    \"\"\"\n",
    "    self.link_extractors = link_extractors\n",
    "\n",
    "  def summarize(self,\n",
    "                documents: List[Document],\n",
    "                *,\n",
    "                summarize_dict: Dict[str, Any] = {}) -> Tuple[List[Document], List[Document]]:\n",
    "    \"\"\"Return the summaries of the communities in the given documents.\n",
    "\n",
    "    If `link_extractors` were specified when constructed, each documuent\n",
    "    in `documents` will have links added based on the selected extractors.\n",
    "\n",
    "    Parameters:\n",
    "      documents: The documents to summarize.\n",
    "      summarize_dict: Dictionary containing additional key/value pairs to\n",
    "        pass to the summarizaiton chain.\n",
    "\n",
    "    Returns:\n",
    "      The a tuple containing the original documents (with additional links, if any) and\n",
    "      the list of community summaries.\n",
    "    \"\"\"\n",
    "\n",
    "    # If necessary, run the link extractors to add links.\n",
    "    if self.link_extractors:\n",
    "        # Run each extractor over all documents.\n",
    "        links_per_extractor = [e.extract_many(documents) for e in self.link_extractors]\n",
    "\n",
    "        # Transpose the list of lists to pair each document with the tuple of links.\n",
    "        links_per_document = zip(*links_per_extractor)\n",
    "\n",
    "        documents = [\n",
    "            copy_with_links(document, *links)\n",
    "            for document, links in zip(documents, links_per_document)\n",
    "        ]\n",
    "\n",
    "    # Generate the communities\n",
    "    communities = group_by_community(documents)\n",
    "\n",
    "    # Summarize each community.\n",
    "    summaries = [self.summarize.invoke({**summarize_dict, \"context\": community }) for community in communities]\n",
    "\n",
    "    return (documents, summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Retrieve Chunks and Summarize Communities\n",
    "\n",
    "Incorporating summarization into a RAG pipeline is useful when there may be a large number of relevant chunks. In this example, we'll retrieve a large number of chunks based on the question and then group them into communities to be summarized.\n",
    "\n",
    "Since this happens \"late\" -- after the question is known -- we're able to use that to ask the summaries be generated in a way that focuses on information relevant to the question. We can then combine the summaries hierarchically to produce the final answer.\n",
    "\n",
    "The implementation uses LangGraph and is based on [this LangChain example showing how to orchestrate summarization using LangGraph](https://python.langchain.com/v0.2/docs/tutorials/summarization/#orchestration-via-langgraph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Create the retriever.\n",
    "# For summarization, we use a higher `k` so we have more chunks.\n",
    "retriever = store.as_retriever(\n",
    "    search_type = \"mmr_traversal\",\n",
    "    search_kwargs = {\n",
    "        \"k\": 20,\n",
    "        \"fetch_k\": 50,\n",
    "        \"depth\": 2,\n",
    "        # \"score_threshold\": 0.2,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Community Summarization using LangGraph\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, List, Literal, TypedDict\n",
    "\n",
    "from langchain.chains.combine_documents.reduce import (\n",
    "    acollapse_docs,\n",
    "    split_list_of_docs,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "token_max = 1000\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "def length_function(documents: List[Document]) -> int:\n",
    "    \"\"\"Get number of tokens for input contents.\"\"\"\n",
    "    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)\n",
    "\n",
    "\n",
    "# This will be the overall state of the main graph.\n",
    "# It will contain the input document contents, corresponding\n",
    "# summaries, and a final summary.\n",
    "class OverallState(TypedDict):\n",
    "    # Notice here we use the operator.add\n",
    "    # This is because we want combine all the summaries we generate\n",
    "    # from individual nodes back into one list - this is essentially\n",
    "    # the \"reduce\" part\n",
    "    question: str\n",
    "    communities: List[List[Document]]\n",
    "    summaries: Annotated[list, operator.add]\n",
    "    collapsed_summaries: List[Document]\n",
    "    final_summary: str\n",
    "\n",
    "\n",
    "# This will be the state of the node that we will \"map\" all\n",
    "# documents to in order to generate summaries\n",
    "class SummaryState(TypedDict):\n",
    "    content: List[str]\n",
    "    question: str\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "map_template = \"\"\"\n",
    "The following is a set of documents:\n",
    "{content}\n",
    "\n",
    "First, describe the common topic in the documents.\n",
    "\n",
    "Then, write a concise summary of that topic based on the documents related to the question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", map_template)]\n",
    ")\n",
    "\n",
    "map_chain = map_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Here we generate a summary, given a document\n",
    "async def generate_summary(state: SummaryState):\n",
    "    response = await map_chain.ainvoke(state)\n",
    "    return {\"summaries\": [response]}\n",
    "\n",
    "\n",
    "# Here we define the logic to map out over the documents\n",
    "# We will use this an edge in the graph\n",
    "def map_summaries(state: OverallState):\n",
    "    # We will return a list of `Send` objects\n",
    "    # Each `Send` object consists of the name of a node in the graph\n",
    "    # as well as the state to send to that node\n",
    "    return [\n",
    "        Send(\"generate_summary\", {\"content\": [doc.page_content for doc in community],\n",
    "                                  \"question\": state[\"question\"]})\n",
    "        for community in state[\"communities\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "def collect_summaries(state: OverallState):\n",
    "    return {\n",
    "        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]\n",
    "    }\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "The following is a set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary\n",
    "of the main themes answering the following question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "\n",
    "reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Add node to collapse summaries\n",
    "async def collapse_summaries(state: OverallState):\n",
    "    doc_lists = split_list_of_docs(\n",
    "        state[\"collapsed_summaries\"], length_function, token_max\n",
    "    )\n",
    "    results = []\n",
    "    for doc_list in doc_lists:\n",
    "        results.append(await acollapse_docs(doc_list,\n",
    "                                            lambda docs: reduce_chain.ainvoke({\n",
    "                                                \"docs\": docs,\n",
    "                                                \"question\": state[\"question\"],\n",
    "                                            })))\n",
    "\n",
    "    return {\"collapsed_summaries\": results}\n",
    "\n",
    "\n",
    "# This represents a conditional edge in the graph that determines\n",
    "# if we should collapse the summaries or not\n",
    "def should_collapse(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "    num_tokens = length_function(state[\"collapsed_summaries\"])\n",
    "    if num_tokens > token_max:\n",
    "        return \"collapse_summaries\"\n",
    "    else:\n",
    "        return \"generate_final_summary\"\n",
    "\n",
    "\n",
    "# Here we will generate the final summary\n",
    "async def generate_final_summary(state: OverallState):\n",
    "    response = await reduce_chain.ainvoke({\n",
    "        \"docs\": state[\"collapsed_summaries\"],\n",
    "        \"question\": state[\"question\"],\n",
    "    })\n",
    "    return {\"final_summary\": response}\n",
    "\n",
    "\n",
    "# Construct the graph\n",
    "# Nodes:\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_summary\", generate_summary)\n",
    "graph.add_node(\"collect_summaries\", collect_summaries)\n",
    "graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
    "graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "\n",
    "# Edges:\n",
    "graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
    "graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
    "graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
    "graph.add_edge(\"generate_final_summary\", END)\n",
    "\n",
    "community_summarizer = graph.compile()\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "summary_retriever = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"communities\": RunnablePassthrough() | retriever | RunnableLambda(group_by_community),\n",
    "    } | community_summarizer | itemgetter(\"final_summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russia, the largest country in the world, is known for its vast and diverse environment, rich history, and significant cultural and geopolitical influence. The landscape features expansive forests, tundras, and mountain ranges, with climates ranging from the frigid Arctic in the north to more temperate zones in the south. Russia is abundant in natural resources like oil, natural gas, and minerals but faces environmental challenges such as pollution, deforestation, and climate change.\\n\\nHistorically, Russia has seen significant territorial expansion, especially during the Tsardom and Soviet eras, becoming a multi-ethnic state. The Soviet Union (1922-1991) was a major global power, playing crucial roles in World War II and the Cold War, and achieving technological milestones like launching the first human-made satellite and sending the first humans into space.\\n\\nCulturally, Russia is deeply influenced by the Russian language and Orthodox Christianity, with roots tracing back to Kievan Rus\\'. Ethnic Russians are the largest ethnic group in Europe, with notable minorities in former Soviet states and a large diaspora worldwide. Modern Russia continues to be a major player on the world stage, with complex relationships with its post-Soviet neighbors and ongoing disputes over former Soviet properties. Nationalist ideologies, such as \"Russia for Russians,\" reflect internal debates over multiculturalism and ethnic identity.\\n\\nPolitically, Russia has a bicameral parliament consisting of the State Duma and the Federation Council, where MPs create and pass laws. Significant cities like Rostov-on-Don serve as important administrative and economic centers, highlighting Russia\\'s regional diversity and strategic importance.\\n\\nAdditionally, Russia offers recreational and social facilities such as the Moscow Country Club, a golf club that exemplifies the type of amenities available in the country. The year 2000 is noted for its global significance, including the Y2K problem, its designation as the International Year for the Culture of Peace, and the World Mathematical Year.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await summary_retriever.ainvoke(\"Tell me about Russia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Summarization is useful in a variety of places beyond requests to summarize a dataset. During indexing, summaries can be persisted and retrieved separately or used to filter retrieved content to documents whose summary is closest to the question. Summaries can also be used for computing other embeddings -- for instance, a document could be subdivided into chunks and communities, with the summary of each community being used to create an embedding for a multi-vector retriever case. During retrieval, if the retrieved content is too large it can be useful to summarize the retrieval results and use the summary to answer the question.\n",
    "\n",
    "In this post we demonstrated the application of community-based summarization to the graph of links between documents. This is an easy way to increase the quality of summarization steps in your LangChain applications, and can be used with or without a `GraphVectorStore`. It is possible to extract links from any document chunk – for instance, based on the links and/or structure of the documents – and use those links to generate and summarize communities. This provides the benefits of local community summarization without needing to extract a knowledge graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
